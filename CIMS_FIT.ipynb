{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIMS FIT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkk-n9Sb88G8"
      },
      "source": [
        "# Temporal Hierarchical Bayesian Causal Inference in Audiovisual Speech Perception\n",
        "\n",
        " - Here we are implementing a modified version of the CIMS model, as first outlined by Magnotti et al., (2013) --> https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00798/full\n",
        " - This specific implementation is testing the predictions of two decision functions, model selection & probability matching, as outlined in Wozny et al., (2010) --> https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000871\n",
        " - Explanations of model derivation, fitting and comparison can be found on the associated github repository --> https://github.com/julesneuro/BCI-Audiovisual-Speech\n",
        " - The entire programme is very computationally expensive due to the trial-wise sampling, I've only tried to run it using colab so I'm sure it'd perform much better on a dedicated workstation. It's also the first computational model of this size I've ever written - so a big learning curve!\n",
        " - At the end of the day each optimization attempt takes about 10 minutes for each model (thus 20 minutes per participant) because of the extensive parameter search conducted in the basinhopping algorithm (and the limitations of google colab). I'd like to parrellise this at some point but I've not had time yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI2reufdQbg4"
      },
      "source": [
        "# TRIED RETURNING PREDS BUT IT JUST DOESN'T WORK\n",
        "# FIXED HALF THE LOGLIKELIHOOD PROBLEM\n",
        "# PERHAPS TRY WITHOUT THE POOLED LOGLIKELIHOOD PERHAPS IT MAKES IT MUCH MORE COMPLICATED?\n",
        "# tRY WITH FIXED SN\n",
        "# TRY WITHOUT BOUNDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcbHsSz9BeZ"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from numpy import random\n",
        "from scipy.stats import norm, binom\n",
        "from scipy.optimize import minimize, Bounds, brute, differential_evolution, basinhopping\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUH7GBXR87HJ",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f1250660-ec3f-4331-f1c6-28ba8141b6fd"
      },
      "source": [
        "# IMPORT DATA\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7895f83a-66bd-45bd-a705-deade12340a2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7895f83a-66bd-45bd-a705-deade12340a2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AVI_ProjSimData.xlsx to AVI_ProjSimData.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzIAlwBFcdgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbbc04b-5b3b-4a64-d173-a616362c4bf6"
      },
      "source": [
        "# EXPERIMENTAL PARAMETERS\n",
        "async_conditions = [-300, -267, -200, -133, -100, -67, 0, 67, 100, 133, 200, 267, 300, 400, 500]\n",
        "n_trials = 180 # NEEDS TO BE CHANGED FOR OUR EXPERIMENT\n",
        "trials_per_cond = int(n_trials / len(async_conditions)) \n",
        "\n",
        "# PARAMETERS\n",
        "pc1 = 0.58\n",
        "sens_noise = 100\n",
        "mu1 = 0.0\n",
        "sigma1 = 50\n",
        "mu2 = -48\n",
        "sigma2 = 123\n",
        "\n",
        "exp_params = [async_conditions, n_trials, trials_per_cond]\n",
        "model_params = [pc1, sens_noise, mu1, sigma1, mu2, sigma2]"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trials per condition is set at 12!\n",
            "[0.3, 108, 0.0, 191, -85, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_iiXLQc7sz"
      },
      "source": [
        "# DATAFRAME SETUPS\n",
        "\n",
        "data = pd.read_csv(\"MAGNOTTI DATA.csv\") # CHANGE THIS TO THE REAL DATA\n",
        "data_df = pd.DataFrame(data)\n",
        "\n",
        "ms_param_cols = [\"PC1\", \"SES_NOISE\", \"SIGMA1\", \"MU2\", \"SIGMA2\", \"nLL_MS\", \"R2\"]\n",
        "ms_param_df = pd.DataFrame(data = None, columns = ms_param_cols)\n",
        "\n",
        "pm_param_cols = [\"PC1\", \"SES_NOISE\", \"SIGMA1\", \"MU2\", \"SIGMA2\", \"nLL_PM\", \"R2\"]\n",
        "pm_param_df = pd.DataFrame(data = None, columns = pm_param_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A8pmFPS9nHm"
      },
      "source": [
        "# MODEL FUNCTIONS\n",
        "\n",
        "def calc_posterior(x, cond, model_params):\n",
        "\n",
        "  \"\"\"Args: x, cond, sens_noise\"\"\"\n",
        "\n",
        "  noise = sens_noise / 2\n",
        "\n",
        "  cond = float(cond) # make sure the condition is a float\n",
        "  var1 = sigma1**2 # transform into variance for the cdf function\n",
        "  var2 = sigma2**2\n",
        "  varx = noise**2\n",
        "\t\n",
        "  lprior = 2 * np.log(pc1 / (1-pc1)) \n",
        "  b = np.log((varx + var2) / (varx + var1)) +  (mu2**2 / (var2 - var1))\n",
        "  c = (1 / (varx + var1)) - (1 / (varx + var2))\n",
        "\n",
        "  if lprior < -b:\n",
        "    lprior = -b\n",
        "\n",
        "  bound = np.sqrt((lprior+b)/c)\n",
        "  middle = abs(mu2) * (varx+var1)/(var2-var1)\n",
        "  upper = middle + bound\n",
        "  lower = middle - bound\n",
        "\n",
        "  one = norm.cdf(x = upper, loc = x, scale = noise)\n",
        "  two = norm.cdf(x = lower, loc = x, scale = noise)\n",
        "\n",
        "  posterior = one - two\n",
        "\n",
        "  return posterior\n",
        "\n",
        "def model_selection(c1_posterior): # CHECKED\n",
        "\n",
        "  \"\"\" Args: c1_posterior (float)\"\"\"\n",
        "\n",
        "  if c1_posterior > 0.5:\n",
        "\n",
        "    return 1 \n",
        "\n",
        "  else:\n",
        "\n",
        "    return 0\n",
        "\n",
        "def probability_matching(c1_posterior): \n",
        "\n",
        "  \"\"\" Args: c1_posterior (float)\"\"\"\n",
        "  \n",
        "  rng = np.random.default_rng()\n",
        "  alpha = rng.uniform(low = 0, high = 1)\n",
        "\n",
        "  if c1_posterior > alpha:\n",
        "\n",
        "    return 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    return 0\n",
        "\n",
        "# HELPER FUNCS\n",
        "\n",
        "def prob_converter(preds, trials_per_cond):\n",
        "\n",
        "  \"\"\"Args: preds, trials_per_cond\"\"\"\n",
        "\n",
        "  prob = preds / trials_per_cond\n",
        "\n",
        "  return prob\n",
        "\n",
        "def clipper(x):\n",
        "  # to stop log divisions by zero when using the loglikelihood!\n",
        "  low = 1e-6\n",
        "  high = (1 - 1e-6)\n",
        "\n",
        "  if x < low:\n",
        "    x = low\n",
        "    return x\n",
        "  elif x > high:\n",
        "    x = high\n",
        "    return x\n",
        "  else:\n",
        "    return x\n",
        "\n",
        "def ll_binomial(p, y, n): \n",
        "\n",
        "   \"\"\"Args: p = prob of success, y = success in observed data,\n",
        "  n = n_trials\"\"\"\n",
        "\n",
        "  # log L(y | p;n) = log(C) + log(p)*y + log(1-p)*(n-y)\n",
        "  p = clipper(p)\n",
        "\n",
        "  q = 1-p\n",
        "  s = y * np.log(p)\n",
        "  f = (n-y) * np.log(q)\n",
        "\t\n",
        "  ll = s + f\n",
        "\n",
        "  return (ll)\n",
        "\n",
        "def calc_nll(success_prob, observed_data, trials_per_cond): \n",
        "\n",
        "  \"\"\"Args: success_prob = prob, observed_data = single item,\n",
        "  trials_per_cond = trials per cond\"\"\"\n",
        "\n",
        "  nll = ll_binomial(success_prob, observed_data, trials_per_cond)\n",
        "  \n",
        "  return nll\n",
        "\n",
        "# PREDICT / MAXIMUM LIKELIHOOD EST FUNCTIONS\n",
        "\n",
        "def pred_MS(model_params, exp_params): \n",
        "\n",
        "  \"\"\"Args: arams, exp_params\"\"\"\n",
        "\n",
        "  MS_fpreds = []\n",
        "\n",
        "  noise = sens_noise / 2\n",
        "\n",
        "  for cond in async_conditions:\n",
        "\n",
        "    MS_preds = []\n",
        "\n",
        "    for i in range(trials_per_cond):\n",
        "\n",
        "      x = np.random.normal(loc = cond, scale = noise, size = 1)\n",
        "      posterior = calc_posterior(x, cond, model_params)\n",
        "      MS_pred = model_selection(posterior) # make prediction of synchrony\n",
        "      MS_preds.append(MS_pred)\n",
        "      \n",
        "    MS_fin_preds = sum(MS_preds)\n",
        "    MS_fpreds.append(MS_fin_preds) # append to the relevant list\n",
        "\n",
        "  return MS_fpreds # this should return a list of final predictions \n",
        "\n",
        "def pred_PM(model_params, exp_params):\n",
        "\n",
        "  \"\"\"Args: model_params, exp_params\"\"\"\n",
        "\n",
        "  PM_fpreds = []\n",
        "\n",
        "  noise = sens_noise / 2\n",
        "\n",
        "  for cond in async_conditions:\n",
        "\n",
        "    PM_preds = []\n",
        "\n",
        "    for i in range(trials_per_cond):\n",
        "\n",
        "      x = np.random.normal(loc = cond, scale = noise, size = 1)\n",
        "\n",
        "      posterior = calc_posterior(x, cond, model_params)\n",
        "      PM_pred = probability_matching(posterior) # make prediction of synchrony\n",
        "\n",
        "      PM_preds.append(MS_pred)\n",
        "      \n",
        "    PM_fin_preds = sum(PM_preds)\n",
        "    PM_fpreds.append(PM_fin_preds) # append to the relevant list\n",
        "\n",
        "  return PM_fpreds\n",
        "\n",
        "def calc_nLL_MS(subject_data, MS_fpreds, trials_per_cond): \n",
        "\n",
        "  \"\"\"Args: Subject_data should be converted into an array, w/o participant ID,\n",
        "  MS_fpreds referes to previous list of predictions, trials_per_cond is an exp_param\"\"\"\n",
        "\n",
        "  nLL_list = [] # for computing overall nLL of given params\n",
        "  \n",
        "  for pred, ob in zip(MS_fpreds, subject_data): # for every related item in the list we need to compute the LL \n",
        "               # if this method doesn't work we could always relate them in a dict\n",
        "    prob = prob_converter(pred, trials_per_cond) # calculates probability of a success\n",
        "    cond_nLL = calc_nll(prob, ob, trials_per_cond)\n",
        "    nLL_list.append(cond_nLL) # add it back to the list\n",
        "\n",
        "  nLL_MS = sum(nLL_list) / 15 # exit loop here, return nLL\n",
        "  nLL_MS.astype(float)\n",
        "\n",
        "  return nLL_MS\n",
        "\n",
        "def calc_nLL_PM(subject_data, PM_fpreds, trials_per_cond): \n",
        "\n",
        "  \"\"\"Args: Subject_data should be converted into an array, w/o participant ID,\n",
        "  MS_fpreds referes to previous list of predictions, trials_per_cond is an exp_param\"\"\"\n",
        "\n",
        "  nLL_list = [] # for computing overall nLL of given params\n",
        "  \n",
        "  for pred, ob in zip(PM_fpreds, subject_data): # for every related item in the list we need to compute the LL \n",
        "               # if this method doesn't work we could always relate them in a dict\n",
        "    prob = prob_converter(pred, trials_per_cond) # calculates probability of a success\n",
        "    cond_nLL = calc_nll(prob, ob, trials_per_cond)\n",
        "    nLL_list.append(cond_nLL) # add it back to the list\n",
        "\n",
        "  nLL_PM = sum(nLL_list) / 15 # exit loop here, return nLL\n",
        "  nLL_PM.astype(float)\n",
        "\n",
        "  return nLL_PM\n",
        "\n",
        "def calc_full_nLL_MS(model_params, subject_data, exp_params):\n",
        "  \n",
        "  \"\"\"Args: Subject_data, model_params, exp_params\"\"\"\n",
        "\n",
        "  preds = pred_MS(model_params, exp_params)\n",
        "  nLL_MS = calc_nLL_MS(subject_data, preds, trials_per_cond)\n",
        "  #pred = preds.astype(str)\n",
        "\n",
        "  return nLL_MS\n",
        "\n",
        "def calc_full_nLL_PM(model_params, subject_data, exp_params): \n",
        "\n",
        "  \"\"\"Args: Subject_data, model_params, exp_params\"\"\"\n",
        "\n",
        "  preds = pred_PM(model_params, exp_params)\n",
        "  nLL_PM = calc_nLL_PM(subject_data, preds, trials_per_cond)\n",
        "\n",
        "  return nLL_PM\n",
        "\n",
        "# MODEL OPTIMIZATION \n",
        "\n",
        "def print_fun(x, f, accepted):\n",
        "  print(\" x = %s, f = %s, accepted? %s\" %(x, f, accepted))\n",
        "\n",
        "def optimize_global_MS(model_params, subject_data, exp_params):\n",
        "\n",
        "  \"\"\"Args: model_params list, subject_data as an array, \n",
        "    exp_params as a list\"\"\"\n",
        "\n",
        "  bounds = [(0,1), (0, 250), (0, 250), (-250, 250), (0, 250)]\n",
        "  # PAR ORDER: PC1, SN, SIGMA1, MU2, SIGMA2\n",
        "\n",
        "  rand_prior = np.random.uniform(1e-6, (1 - 1e-6), 1) # between 0 and 1\n",
        "  rand_sn = np.random.uniform(0, 200, 1) \n",
        "  rand_s1 = np.random.uniform(0, 200, 1)\n",
        "  rand_mu2 = np.random.uniform(-300, 300, 1)\n",
        "  rand_s2 = np.random.uniform(0, 200, 1)\n",
        "  x0_array = np.array([rand_prior, rand_sn, rand_s1, rand_mu2, rand_s2])\n",
        "  print(x0_array)\n",
        "  args = (subject_data, exp_params)\n",
        "  opt_result = basinhopping(func = calc_full_nLL_MS, x0 = x0_array, niter = 150,\n",
        "                            minimizer_kwargs = {\"args\":args, \"method\":\"L-BFGS-B\", \"bounds\":bounds}, \n",
        "                            callback = print_fun)\n",
        "  \n",
        "  # need to make sure this is returning the values with the lowest loglikelihood\n",
        "  \n",
        "  fitted_pars_ms = opt_result.x\n",
        "  fitted_preds_ms = opt_result.fun\n",
        "\n",
        "  print(\"It took the CIMS-MS optimizer %s iterations to converge.\" %(opt_result.nit))\n",
        "\n",
        "  return fitted_pars_ms, fitted_preds_ms\n",
        "\n",
        "def optimize_global_PM(model_params, subject_data, exp_params):\n",
        "\n",
        "  \"\"\"Args: model_params list, subject_data as an array, \n",
        "    exp_params as a list\"\"\"\n",
        "\n",
        "  bounds = [(0,1), (0, 250), (0, 250), (-250, 250), (0, 250)]\n",
        "  # PAR ORDER: PC1, SN, SIGMA1, MU2, SIGMA2\n",
        "\n",
        "  rand_prior = np.random.uniform(1e-6, (1 - 1e-6), 1) # between 0 and 1\n",
        "  rand_sn = np.random.uniform(0, 200, 1) \n",
        "  rand_s1 = np.random.uniform(0, 200, 1)\n",
        "  rand_mu2 = np.random.uniform(-300, 300, 1)\n",
        "  rand_s2 = np.random.uniform(0, 200, 1)\n",
        "  x0_array = np.array([rand_prior, rand_sn, rand_s1, rand_mu2, rand_s2])\n",
        "  print(x0_array)\n",
        "  args = (subject_data, exp_params)\n",
        "  opt_result = basinhopping(func = calc_full_nLL_PM, x0 = x0_array, niter = 150,\n",
        "                            minimizer_kwargs = {\"args\":args, \"method\":\"L-BFGS-B\", \"bounds\":bounds}, \n",
        "                            callback = print_fun)\n",
        "  \n",
        "  # need to make sure this is returning the values with the lowest loglikelihood\n",
        "  \n",
        "  fitted_pars_pm = opt_result.x\n",
        "  fitted_preds_pm = opt_result.fun\n",
        "\n",
        "  print(\"It took the CIMS-PM optimizer %s iterations to converge.\" %(opt_result.nit))\n",
        "\n",
        "  return fitted_pars_pm, fitted_preds_pm\n",
        "\n",
        "# FIT MS\n",
        "\n",
        "def fit_MS(subject_data, model_params, exp_params):\n",
        "\n",
        "  \"\"\"Args: subject_data (array), model_params, exp_params\"\"\"\n",
        "  \n",
        "  fitted_pars_ms, fitted_nll_ms = optimize_global_MS(model_params, subject_data, exp_params)\n",
        "\n",
        "  return fitted_pars_ms, fitted_nll_ms\n",
        "\n",
        "def fit_PM(subject_data, model_params, exp_params):\n",
        "\n",
        "  \"\"\"Args: subject_data (array), model_params, exp_params\"\"\"\n",
        "  \n",
        "  fitted_pars_pm, fitted_nll_pm = optimize_global_PM(model_params, subject_data, exp_params)\n",
        "\n",
        "  return fitted_pars_pm, fitted_nll_pm\n",
        "\n",
        "def r2(x, y):\n",
        "\n",
        "  r2_array = np.corrcoef(x = x, y = y)\n",
        "\n",
        "  return r2_array[0,1]\n",
        "\n",
        "def fit_models_participant(data_array, model_params, exp_params):\n",
        "\n",
        "  \"\"\"Args: data_df, model_params, exp_params\"\"\"\n",
        "\n",
        "  ID = data_array() #index\n",
        "  data = data_array() # index\n",
        "\n",
        "  print(\"Fitting CIMS-MS to subject %s\" %(ID))\n",
        "\n",
        "  fitted_pars_ms, fitted_nll_ms = fit_MS(subject_data, model_params, exp_params) # optimize pars\n",
        "  MS_pars_list = [fitted_pars_ms, fitted_nll_ms]\n",
        "  ms_param_df.append(MS_pars_list)\n",
        "  \n",
        "  print(\"Succesfully fitted CIMS-MS to subject %s\" %(ID))\n",
        "  print(\"Fitting CIMS-PM to subject %s\" %(ID))\n",
        "\n",
        "  fitted_pars_pm, fitted_nll_pm = fit_PM(subject_data, model_params, exp_params) \n",
        "  PM_pars_list = [fitted_pars_pm, fitted_nll_pm]\n",
        "  pm_param_df.append(PM_pars_list)\n",
        "  print(PM_preds_list)\n",
        "  print(PM_pars_list)\n",
        "\n",
        "  print(\"Succesfully fitted CIMS-PM to subject %s\" %(ID))"
      ],
      "execution_count": 116,
      "outputs": []
    }
  ]
}